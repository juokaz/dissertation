\chapter{REST}

\section{What is REST}

Big part of modern applications' requirements is how RESTful they are. REST[], REpresentational State Transfer, was defined in 2000 by Roy Fielding, as a style of application architecture bound by these constraints:

\begin{description}
  \item[Clientâ€“server] A uniform interface separates clients from servers, clients are not concerned with data storage and servers are not concerned with the user interface or user state, so that servers can be simpler and more scalable
  \item[Stateless] No client context is being stored on the server between requests. Each request from any client contains all of the information necessary to service the request, and any session state is held in the client
  \item[Cacheable] Clients can cache responses. Responses must therefore, implicitly or explicitly, define themselves as cacheable, or not, to prevent clients reusing stale or inappropriate data in response to further requests
  \item[Layered system] A client cannot ordinarily tell whether it is connected directly to the end server, or to an intermediary along the way
  \item[Code on demand (optional)] Servers are able temporarily to extend or customize the functionality of a client by the transfer of executable code
  \item[Uniform interface] The uniform interface between clients and servers simplifies and decouples the architecture, which enables each part to evolve independently
\end{description}

When a web application is called RESTful, it has a certain amount of resources, all identified by a global identifier, which can be manipulated over a standardized interface, and exchange representations of them in a format accepted by a client, assuming there exists one. Even though REST style doesn't define this as being HTTP-only, for our purposes, let's assume that all definitions below, are related to HTTP, the protocol web applications talk in. Web application resources would be identified by their URIs, the interface allowing to manipulate them is obviously HTTP and representations would come as different text types.

It thus expected that an interface would behave like this:

\begin{codelisting}
\begin{verbatim}
$ curl -H "Accept: application/json" -X POST -d "name=iphone&title=Iphone5" http://localhost/products
{'name':'Iphone5', 'location': 'http://localhost/products/iphone'}

$ curl -H "Accept:application/html" http://localhost/products/iphone
<html><body>Iphone5</body></html>

$ curl -H "Accept:application/xml" http://localhost/products/iphone
<product><name>Iphone5</name></product>

$ curl -H "Accept:application/json" http://localhost/products/iphone
{'name':'Iphone5'}

$ curl -H "Accept:text/plain"localhost/products/iphone
Iphone5
\end{verbatim}
\end{codelisting}

Even though a usual web application in most cases is just a website, thus it has only representation - HTML - I would expect a modern web application framework or a language to support RESTfullness, as it solves quite a lot of problems in current web state. First of all, it allows any website to become machine readable easily, just by exchanging a different representation, say XML, than the one returned to browsers. Second, the way it defines constrains for cacheability, statelessness and layered systems transparency, it makes web applications scale better, both in terms of development speed and ease and also performance. 

Most importantly, I think Hypermedia[] is crucial part of web as a whole, and for it to evolve for years to come, all web endpoints, being simple websites or machine consumable APIs, have to support it. It is not a big requirement for a certain web framework or a language, there are only two things it needs to do well - allow to expose resources identifiable by unique IDs, which should be predictable and reliable, and handle different response types, not just HTML. 

\section{URL structures}

Even though this is not a requirement of REST style, which allows any URLs as long as they are Unique IDentifiers (UIDs), usually RESTful web applications expose resources in a manner which is easy for humans to understand. This is achieved by creating an \textit{illusion} that there is a folder structure in the application, which can be navigated upwards or downwards to retrieve parent/child elements. Obviously they are rarely real folders and files, but all requests are caught by a web server and passed to a handling script, which would then parse them and figure out how to handle it. An example structure might look like this:

\begin{description}
  \item[/products] \hfill \\
  \textbf{GET}     - List of products \\
  \textbf{POST}    - Create a product
  \item[/products/iphone] \hfill \\
  \textbf{GET}     - Product resource \\
  \textbf{PUT}     - Update a product \\
  \textbf{DELETE}  - Delete a product
  \item[/products/iphone/accessories] \hfill \\
  \textbf{GET}     - List of accessories for a product iphone \\
  \textbf{POST}    - Create an accessory for a product iphone
  \item[etc \ldots]
\end{description}

As showed in Requests handling section, Sinatra allows to configure these URLs out of the box, and most of PHP frameworks would do the same. Furthermore, out of the box, most MVC come with the URL structure of \textit{/controller/action} which allows to generate better URLs out of the box. The URL would still include keywords like \textit{view} or \textit{edit}, which are not resources themselves but just different representation, however this is still better than a cryptic URL like \textit{index.php?show=product\&id=325}. These sort of URLs would be something a PHP developer starts with and obviously doesn't much much semantic meaning.

Links, however, has no notion of this as it only allows to move between functions in a sense of a Links language level function and because of the fact that it needs to transfer application state to support that, even for simple things, it will generate URLs itself, which have no semantic meaning to a user. Also Links seem to only handle \textit{GET} and \textit{POST} verbs, but also a developer has no control of how they differentiate, only that for forms, they would post the data rather than passing that as part of query, which is not used as that would require encoding the state.

As explained before, Links stores application state in each URL too, thus the URL it generates for a specific actions depends on previous actions user might have taken. If one entered some information into a form which then got transmitted across as say a search order, it would change the URL of a resource, the products list, even though the resource URI should have stayed the same, only the filtering parameters have changed. Obviously because everything is stored as an encrypted string, user might end up with a lot of different URLs in his browser history without a way to decide which one was the starting page of a search.

Although the biggest shortcoming of Links is that it doesn't produce deterministic URLs. Every time application is recompiled Links changes the URLs unrecognisably as they appear to be encoded also including a timestamp or a random number, which changes. This makes it really hard for an application to be used for creating RESTful applications as it brakes quite a few of it's principles. Responses cannot be cached, as they might have links to other resources which might have changed their URLs because of recompilation, it would be hard to distribute the application between multiple servers since different versions of it, if compiled on different machines, would have different URLs, etc \ldots. At the end, human user cannot bookmark or share an URL to the application as it has unpredictable longevity. 

\section{Hypermedia}

Part of support Hypermedia and thus HATEOAS[] is being able to handle different response representation types depending on what user is requesting for. When an application has this, any resource can be accessed by a human user and machine reading software just by using different representations. The important fact here is that resources should always stay the same and only their representations should differ, thus application can evolve and gain various other advantages[]. To support this behaviour, a modified Sintara example would look this (\textit{nakogiri} is a Ruby library which handles Xml parsing and generation, for purposes of this report it is not important how it behaves):

\begin{codelisting}
\begin{verbatim}
get '/products/:id*' do
  data = { :id => params[:id] }

  request.accept.each do |type|
    case type
    when 'text/html'
      halt erb "<h1>#{data.id}</h1>"
    when 'text/json'
      halt data.to_json
    when 'application/atom+xml'
      halt nokogiri(:'index.atom', :locals => data)
    when 'application/xml', 'text/xml'
      halt nokogiri(:'index.xml', :locals => data)
    when 'text/plain'
      halt data.name
    end
  end
  error 406
end
\end{verbatim}
\end{codelisting}

In this example same handler would retrieve the data from some data-store which is then displayed depending on what \textit{Accept} header was passed to an application. PHP would support this behaviour quite similarly, there is a global request variable called \textit{\$\_SERVER['HTTP\_ACCEPT']} which contains the same \textit{Accept} header as the one in Sinatra example. Types in example above include text, html, JSON and various Xml types. If a type is not found, \textit{HTTP Error 406 Not acceptable} is returned to indicate to a user, being a human or a machine, that the type he is requesting in is either not supported or he hasn't asked for a specific type. 

Because of Links model of handling requests, this is also not supported. Although it would be easily supportable if a function invoked by a click would get passed request headers information, then it would be able to parse the \textit{Accept} header itself and generate different outputs depending on that. Links applications are completely not aware of HTTP environment they are being used in and also can't modify it in any way (for other functionality, to be able to add additional headers for example), this makes applications really limited in how they can behave, and in my belief is wrong as it's abstraction HTTP so much to the point where key functionality is lost. 

\section{Improving Links}

As per report goals, Links cannot be used for RESTful applications, which is a big drawback given that most of other languages support this. Even though the changes required to support this would be core language changes, I think this is necessary to make the language ready for modern applications. 

The goal of this modification is to gain functionality of being able to modify URLs Links generates for server-side function calls. It could still generate encrypted URLs for functions which do not explicitly define the URL format, but for those who do, it should use that as a template. A template should have as much parameters as function definition, so it could be type-checked and also URLs generated from it. For example, a proposed syntax would look like this:

\begin{codelisting}
\begin{verbatim}
# Current syntax
fun lookupUserID(username) server {
    for (user <- users)
    where (user.name = username)
      [user.id]
  }
# lookupUserId('juozas') -> blog.links?\_k=BAH7BgH6BgQBxgwBxQwCAgJpZAAAAwYxY=

# Proposed syntax
fun lookupUserID(username) server('/users/:username') {
    for (user <- users)
    where (user.name = username)
      [user.id]
  }
# lookupUserId('juozas') -> /users/juozas
\end{verbatim}
\end{codelisting}
