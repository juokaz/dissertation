\chapter{REST}

\section{What is REST}

\section{URL structures}

Even though this is not a requirement of REST style, which allows any URLs as long as they are Unique IDentifiers (UIDs), usually RESTful web applications expose resources in a manner which is easy for humans to understand. This is achieved by creating an \textit{illusion} that there is a folder structure in the application, which can be navigated upwards or downwards to retrieve parent/child elements. Obviously they are rarely real folders and files, but all requests are caught by a web server and passed to a handling script, which would then parse them and figure out how to handle it. An example structure might look like this:

\begin{description}
  \item[/products] \hfill \\
  \textbf{GET}     - List of products \\
  \textbf{POST}    - Create a product
  \item[/products/iphone] \hfill \\
  \textbf{GET}     - Product resource \\
  \textbf{PUT}     - Update a product \\
  \textbf{DELETE}  - Delete a product
  \item[/products/iphone/accessories] \hfill \\
  \textbf{GET}     - List of accessories for a product iphone \\
  \textbf{POST}    - Create an accessory for a product iphone
  \item[etc \ldots]
\end{description}

As showed in Requests handling section, Sinatra allows to configure these URLs out of the box, and most of PHP frameworks would do the same. Furthermore, out of the box, most MVC come with the URL structure of \textit{/controller/action} which allows to generate better URLs out of the box. The URL would still include keywords like \textit{view} or \textit{edit}, which are not resources themselves but just different representation, however this is still better than a cryptic URL like \textit{index.php?show=product\&id=325}. These sort of URLs would be something a PHP developer starts with and obviously doesn't much much semantic meaning.

Links, however, has no notion of this as it only allows to move between functions in a sense of a Links language level function and because of the fact that it needs to transfer application state to support that, even for simple things, it will generate URLs itself, which have no semantic meaning to a user. Also Links seem to only handle \textit{GET} and \textit{POST} verbs, but also a developer has no control of how they differentiate, only that for forms, they would post the data rather than passing that as part of query, which is not used as that would require encoding the state.

As explained before, Links stores application state in each URL too, thus the URL it generates for a specific actions depends on previous actions user might have taken. If one entered some information into a form which then got transmitted across as say a search order, it would change the URL of a resource, the products list, even though the resource URI should have stayed the same, only the filtering parameters have changed. Obviously because everything is stored as an encrypted string, user might end up with a lot of different URLs in his browser history without a way to decide which one was the starting page of a search.

Although the biggest shortcoming of Links is that it doesn't produce deterministic URLs. Every time application is recompiled Links changes the URLs unrecognisably as they appear to be encoded also including a timestamp or a random number, which changes. This makes it really hard for an application to be used for creating RESTful applications as it brakes quite a few of it's principles. Responses cannot be cached, as they might have links to other resources which might have changed their URLs because of recompilation, it would be hard to distribute the application between multiple servers since different versions of it, if compiled on different machines, would have different URLs, etc \ldots. At the end, human user cannot bookmark or share an URL to the application as it has unpredictable longevity. 

\section{Hypermedia}

Part of support Hypermedia and thus HATEOAS[] is being able to handle different response representation types depending on what user is requesting for. When an application has this, any resource can be accessed by a human user and machine reading software just by using different representations. The important fact here is that resources should always stay the same and only their representations should differ, thus application can evolve and gain various other advantages[]. To support this behaviour, a modified Sintara example would look this (\textit{nakogiri} is a Ruby library which handles Xml parsing and generation, for purposes of this report it is not important how it behaves):

\begin{codelisting}
\begin{verbatim}
get '/products/:id*' do
  data = { :id => params[:id] }

  request.accept.each do |type|
    case type
    when 'text/html'
      halt erb "<h1>#{data.id}</h1>"
    when 'text/json'
      halt data.to_json
    when 'application/atom+xml'
      halt nokogiri(:'index.atom', :locals => data)
    when 'application/xml', 'text/xml'
      halt nokogiri(:'index.xml', :locals => data)
    when 'text/plain'
      halt data.name
    end
  end
  error 406
end
\end{verbatim}
\end{codelisting}

In this example same handler would retrieve the data from some data-store which is then displayed depending on what \textit{Accept} header was passed to an application. PHP would support this behaviour quite similarly, there is a global request variable called \textit{\$\_SERVER['HTTP\_ACCEPT']} which contains the same \textit{Accept} header as the one in Sinatra example. Types in example above include text, html, JSON and various Xml types. If a type is not found, \textit{HTTP Error 406 Not acceptable} is returned to indicate to a user, being a human or a machine, that the type he is requesting in is either not supported or he hasn't asked for a specific type. 

Because of Links model of handling requests, this is also not supported. Although it would be easily supportable if a function invoked by a click would get passed request headers information, then it would be able to parse the \textit{Accept} header itself and generate different outputs depending on that. Links applications are completely not aware of HTTP environment they are being used in and also can't modify it in any way (for other functionality, to be able to add additional headers for example), this makes applications really limited in how they can behave, and in my belief is wrong as it's abstraction HTTP so much to the point where key functionality is lost. 

\section{Improving Links}

As per report goals, Links cannot be used for RESTful applications, which is a big drawback given that most of other languages support this. Even though the changes required to support this would be core language changes, I think this is necessary to make the language ready for modern applications. 

\subsection{URL generation}

The goal of this modification is to gain functionality of being able to modify URLs Links generates for server-side function calls. It could still generate encrypted URLs for functions which do not explicitly define the URL format, but for those who do, it should use that as a template. A template should have as much parameters as function definition, so it could be type-checked and also URLs generated from it. For example, a proposed syntax would look like this:

\begin{codelisting}
\begin{verbatim}
# Current syntax
fun lookupUserID(username) server {
    for (user <- users)
    where (user.name = username)
      [user.id]
  }
# lookupUserId('juozas') -> blog.links?\_k=BAH7BgH6BgQBxgwBxQwCAgJpZAAAAwYxY=

# Proposed syntax
fun lookupUserID(username) server('/users/:username') {
    for (user <- users)
    where (user.name = username)
      [user.id]
  }
# lookupUserId('juozas') -> /users/juozas
\end{verbatim}
\end{codelisting}

\subsection{Request access}


