\chapter{REST}

\section{What is REST}

\section{URL structures}

Even though this is not a requirement of REST style, which allows any URLs as long as they are Unique IDentifiers (UIDs), usually RESTful web applications expose resources in a manner which is easy for humans to understand. This is achieved by creating an \textit{illusion} that there is a folder structure in the application, which can be navigated upwards or downwards to retrieve parent/child elements. Obviously they are rarely real folders and files, but all requests are caught by a web server and passed to a handling script, which would then parse them and figure out how to handle it. An example structure might look like this:

\begin{description}
  \item[/products] \hfill \\
  \textbf{GET}     - List of products \\
  \textbf{POST}    - Create a product
  \item[/products/iphone] \hfill \\
  \textbf{GET}     - Product resource \\
  \textbf{PUT}     - Update a product \\
  \textbf{DELETE}  - Delete a product
  \item[/products/iphone/accessories] \hfill \\
  \textbf{GET}     - List of accessories for a product iphone \\
  \textbf{POST}    - Create an accessory for a product iphone
  \item[etc \ldots]
\end{description}

To support this, different framework handle this differently. MVC architecture would have a routing component which maps a list of routes like above to specific controllers' actions. Actions themselves would handle different HTTP verbs or this would be part of route definition. Sintara framework, similarly to a lot of small web frameworks, solves this problem by directly matching a route and verb combo to a certain handler, which is called assuming that combo is satisfied. Example below shows a Sintara application blueprint which internally just modifies Sinatra's routing list to add new string routes to match against:

\begin{codelisting}
\begin{verbatim}
get '/products/:id*' do
   "Iphone"
end

post '/products/:id*' do
   id = params[:id]
   # Handle update
end

delete '/products/:id*' do
   id = params[:id]
   # Delete delete by id
end
\end{verbatim}
\end{codelisting}

This allows to create new resources really quickly and quite clearly indicates what requests it accepts. PHP as a language doesn't help with this in any way, because of the nature of it being just a script, where Ruby applications are small web-servers by themselves. In case of PHP, a web-server of choice must redirect all requests matching non-files (so images, CSS and JavaScripts can still be accessed) to one handler script which parses the passed route to decide a handler to execute. Most of PHP frameworks do this out of the box, usually by employing a routing mechanism described above and redirection of requests to a handling script is achieved by tools like \textit{mod\_rewrite}[] in Apache.

Links, however, has no notion of this as it only allows to move between functions in a sense of a Links language level function and because of the fact that it needs to transfer application state to support that, even for simple things, it will generate URLs itself, which have no semantic meaning to a user. Also Links seem to only handle \textit{GET} and \textit{POST} verbs, but also a developer has no control of how they differentiate, only that for forms, they would post the data rather than passing that as part of query, which is not used as that would require encoding the state.

Although the biggest shortcoming of Links is that it doesn't produce deterministic URLs. Every time application is recompiled Links changes the URLs unrecognisably as they appear to be encoded also including a timestamp or a random number, which changes. This makes it really hard for an application to be used for creating RESTful applications as it brakes quite a few of it's principles. Responses cannot be cached, as they might have links to other resources which might have changed their URLs because of recompilation, it would be hard to distribute the application between multiple servers since different versions of it, if compiled on different machines, would have different URLs, etc \ldots. At the end, human user cannot bookmark or share an URL to the application as it has unpredictable longevity. 

\section{Hypermedia}

Part of support Hypermedia and thus HATEOAS[] is being able to handle different response representation types depending on what user is requesting for. When an application has this, any resource can be accessed by a human user and machine reading software just by using different representations. The important fact here is that resources should always stay the same and only their representations should differ, thus application can evolve and gain various other advantages[]. To support this behaviour, a modified Sintara example would look this (\textit{nakogiri} is a Ruby library which handles Xml parsing and generation, for purposes of this report it is not important how it behaves):

\begin{codelisting}
\begin{verbatim}
get '/products/:id*' do
  data = { :id => params[:id] }

  request.accept.each do |type|
    case type
    when 'text/html'
      halt erb "<h1>#{data.id}</h1>"
    when 'text/json'
      halt data.to_json
    when 'application/atom+xml'
      halt nokogiri(:'index.atom', :locals => data)
    when 'application/xml', 'text/xml'
      halt nokogiri(:'index.xml', :locals => data)
    when 'text/plain'
      halt data.name
    end
  end
  error 406
end
\end{verbatim}
\end{codelisting}

In this example same handler would retrieve the data from some data-store which is then displayed depending on what \textit{Accept} header was passed to an application. PHP would support this behaviour quite similarly, there is a global request variable called \textit{\$\_SERVER['HTTP\_ACCEPT']} which contains the same \textit{Accept} header as the one in Sinatra example. Types in example above include text, html, JSON and various Xml types. If a type is not found, \textit{HTTP Error 406 Not acceptable} is returned to indicate to a user, being a human or a machine, that the type he is requesting in is either not supported or he hasn't asked for a specific type. 

Because of Links model of handling requests, this is also not supported. Although it would be easily supportable if a function invoked by a click would get passed request headers information, then it would be able to parse the \textit{Accept} header itself and generate different outputs depending on that. Links applications are completely not aware of HTTP environment they are being used in and also can't modify it in any way (for other functionality, to be able to add additional headers for example), this makes applications really limited in how they can behave, and in my belief is wrong as it's abstraction HTTP so much to the point where key functionality is lost. 

\section{Improving Links}

As per report goals, Links cannot be used for RESTful applications, which is a big drawback given that most of other languages support this. Even though the changes required to support this would be core language changes, I think this is necessary to make the language ready for modern applications. 

\subsection{URL generation}

The goal of this modification is to gain functionality of being able to modify URLs Links generates for server-side function calls. It could still generate encrypted URLs for functions which do not explicitly define the URL format, but for those who do, it should use that as a template. A template should have as much parameters as function definition, so it could be type-checked and also URLs generated from it. For example, a proposed syntax would look like this:

\begin{codelisting}
\begin{verbatim}
# Current syntax
fun lookupUserID(username) server {
    for (user <- users)
    where (user.name = username)
      [user.id]
  }
# lookupUserId('juozas') -> blog.links?\_k=BAH7BgH6BgQBxgwBxQwCAgJpZAAAAwYxY=

# Proposed syntax
fun lookupUserID(username) server('/users/:username') {
    for (user <- users)
    where (user.name = username)
      [user.id]
  }
# lookupUserId('juozas') -> /users/juozas
\end{verbatim}
\end{codelisting}

\subsection{Request access}
