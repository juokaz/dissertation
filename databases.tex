%% Sample chapter file, for use in a thesis.
%% Don't forget to put the \chapter{...} header onto each file.

\chapter{Databases}

It's very unusual for a modern web application to be running without a database – unless they work as computation APIs, with which you can query for a specific result which is then computed on-a-fly, web applications read data from a certain data-store and represent that in a human, or machine, readable way.

\section{Databases in Links}

Databases functionality is one of the core features of Links language and supports MySQL, PostgreSQL and SQLite. It is implemented and uses language-level constructs to define databases and tables, which then can be queried as if they were local lists of data, differently from other environments where SQL queries have to be written manually. For example to define a blog database and posts' table, one would use code like this:

\begin{codelisting}
\begin{verbatim}
var db = database "blog"; 

var posts = table "posts" with ( 
    id: Int, 
    title: String, 
    summary: String, 
    text: String 
) from db;
\end{verbatim}
\end{codelisting}

Querying data is exposed through a special "\textit{\textless- -}" operator, as "\textit{\textless-}" requires a list (it's possible to convert table handler to a list by using \textit{asList()} method, but then all the data filtering would in application code rather than in a database server, potentially requiring to read millions of rows to memory), used mainly in for loops. This dynamically generates a SQL query to retrieve required records. Each record is returned as a string-indexed tuple, which are strongly typed from definitions of tables. To retrieve all records from \textit{posts} table a developer would have to have something like this:

\begin{codelisting}
\begin{verbatim}
for (var post <−− posts)
    # post.id, post.title, etc.
    [post];
\end{verbatim}
\end{codelisting}

This makes Links much more easier to use for a developer who is not familiar with databases, because then he doesn't need to know how to construct a specific SQL query to retrieve this. Because these queries are generated in compile time, Links compiler can optimize them specifically for a used database server, although it's unclear how many of this is being done. At least adding \textit{where} clauses don't make filtering in application scope, but rather modify the SQL query, thus it's more efficient than what would be a trivial approach.

Second, because SQL queries are not constructed by a developer, they are much harder to be exploitable by SQL injection\citep{sql-injection} attacks, which are briefly attacks where a parameter to be used in a query, for example a username, is given as a portion of SQL query, which if used without checking rewrites the main SQL query to do different things than originally planned. There is no direct manipulation of SQL and Links knows how to insert \textit{where} parameters without allowing a SQL query to be rewritten. This makes application much more secure out of the box than most languages and also protects new developers from creating back-doors without realising this. 

\section{Handling databases in modern frameworks}

\subsection{Ruby}

It's quite common in Ruby wep applications to use active record\citep{map-reduce} pattern based database access, which is one example of an ORM, as provided by a library called \textit{active\_record}. Models are defined as classes extending a base ORM record class, thus giving access to saving and retrieving data from a database in OOP way. Furthermore, by defining a schema for a specific table, this library can also generate a database scheme and then once the application is launched that would be executed in a current connection. This makes managing databases much easier and allows a developer to focus on application logic and let \textit{active\_record} handle the database.

\begin{codelisting}
\begin{verbatim}
require 'active_record'

ActiveRecord::Base.establish_connection(
    :adapter  => "mysql",
    :host     => "localhost",
    :username => "root",
    :password => "root",
    :database => "blog"
)

begin
  ActiveRecord::Schema.define do
    create_table :posts do |t|
      t.string :title
      t.text :summary
      t.text :text
    end
  end
rescue ActiveRecord::StatementInvalid
end

class Post < ActiveRecord::Base
  has_many :comments
  def permalink; "/posts/#{to_param}"; end
end
\end{verbatim}
\end{codelisting}

Example above defines a sample posts schema definition and a model using it, base model classes reads table definition from a database and add those properties to otherwise empty \textit{Post} model. Similarly to Links, this library allows to define data structures in language types, rather than database types and then handles type conversion for input and output itself. Rather than manually executing SQL, \textit{active\_record} allows to access data using query generation methods, which are again database server agnostic, like this:

\begin{codelisting}
\begin{verbatim}
Post.find(:all, :limit => 20, :order => "id DESC").each do |p|
   # handle each p (post)
end
\end{verbatim}
\end{codelisting}

This looks quite similar to how Links would solve this problem, however Links doesn't provide any way to modify the query, it only allows to iterate over tables and then creates queries itself. \textit{active\_record} also allows to then access internal relationships, without having to explicitly query for them, with code like \textit{    pcomments.each}. Obviously if a join didn't happen before, this library will have to execute additional SQL query(-ies) on a fly, but it allows to add join to queries really easily, differently from how Links works.

Active Record is known to be memory heavy and relatively slow as there is a substantial overhead with instantiating model instances for every single of a table, especially with model classes being quite complex because of the logic they extend. Some developers also argue that databases should not be treated as something what could be directly mapped on native language types and SQL is the right way to talk to them. This discussion, sometimes even calling ORM an anti-pattern\citep{antiorm}, has fair points, however Ruby frameworks allow to create applications really fast and optimize them if they become slow, bringing the best from both worlds.

\subsection{PHP}

PHP developers tend to use databases in a quite low-level way - rather that using an ORM or any SQL generator, it's really common to just handle SQL in application level and read data as it is. Obviously there exists ORMs for PHP too, some of them being at the level of Ruby's \textit{active\_record} , but it seems that PHP developers are just more used to handling databases at the lowest level possible. This makes it really easy to execution any behaviour and obviously optimize if an application becomes slow as there is no external code generating database interactions.

\begin{codelisting}
\begin{verbatim}
$db = mysql_connect('localhost', 'root', 'root');
mysql_select_db('blog');

$result = mysql_query("SELECT * FROM posts ORDER BY id DESC LIMIT 0,10");

foreach (mysql_fetch_assoc($result) as $post) {
  // handle each $post
}
\end{verbatim}
\end{codelisting}

Code above is really self-explanatory, once a connection is established, a simple SQL query is executed resulting in a MySQL resource. That resource in this case is fetch as an associative array, however it can also be fetched, or rather converted, as objects or singular values/rows. However this code is database server specific, only working with MySQL servers. PHP has PDO abstraction layer, which allows to use the same code to connect to different types of servers, however it doesn't try to generate SQL queries, thus it isn't a direct replacement for an ORM.

It's really common for a newcomer to PHP to create insecure applications, because they tend to be not aware about SQL Injection. Because the language doesn't aid in any way, an application which is fully open for a hacker to manipulate is easy to create. Modifying the example above, adding support for search (finding blog posts containing the required string in their titles) can be implemented like this:

\begin{codelisting}
\begin{verbatim}
$db = mysql_connect('localhost', 'root', 'root');
mysql_select_db('blog');

$search = $_GET['search'];
$result = mysql_query("SELECT * FROM posts WHERE tile LIKE '%{$search}%' ORDER BY id DESC LIMIT 0,10");

foreach (mysql_fetch_assoc($result) as $post) {
  // handle each $post
}
\end{verbatim}
\end{codelisting}

If this script is requested with "\textit{search=\%` OR 1=1 OR title LIKE `\%}", SQL executed is \textit{SELECT * FROM posts WHERE tile LIKE `\%\%` OR 1=1 OR title LIKE `\%\%` ORDER BY id DESC LIMIT 0,10}, thus an attacker can successfully request any data from the table or do many more tasks, even deleting databases or data. It's not uncommon for PHP to receive a lot of criticism for this, because even though it provides methods to escape external data to be used in SQL queries, a lot of examples in documentation, books and online examples do not use it. Nor use prepared statements which would avoid this problem altogether.

\section{Shortcomings of Links}

Updating database records requires more code than necessary as Links requires for all the table fields to be specified, even if only one is to be updated. This is more prone to errors, reduces code clarity and also makes complex tables cumbersome to manage. \textit{UPDATE} syntax only requires new data to be supplied, thus it's really unclear why it was implemented as this. Additional table definitions can be added to only include a subset of fields, however this could quickly become a management issue as adding definitions for all the different update cases can result in a lot of different sets of fields.

Even though databases and tables are defined in an abstract way, which looks really similar to how ORMs would define them, however it's only a one-way definition. Thus meaning that data can be read from a database and mapped to language types, but those definitions cannot be used to manage the database structure itself. This is something which makes it more likely to be prone to errors when a certain database is out of sync with the application definitions for it. Developer is hence required to make sure that every time he changes the database or application, he updates the other one. Theoretically this is not a big problem, however practically, with development team bigger than one developer and with multiple servers being used for development and production, it can become tricky to manage this. 

From Links documentation it was impossible to decide whether Links supports Unicode (for example UTF-8) databases and if it does, how. Every major website is now using UTF-8 to represent HTML in as with global internet accessibility, content might be coming from various different languages. Databases are then used with a Unicode type columns or as a global setting for a database itself, for example MySQL supports \textit{utf8\_general\_ci} column charset type which makes it store and handle Unicode data. Both PHP and Ruby extensions handle this by default and this something Links has to be doing if it is to be used for modern applications.

\subsection{Slow-SQL generation}

Rather than allowing to construct a query, Links provides iteration mechanisms to fetch data from a database. Similarly to ORMs shortcomings, this is both potentially slow and developer friendly. The issue is best showcased by this piece of code, a query to fetch all comments written on the users' blog posts:

\begin{codelisting}
\begin{verbatim}
var current_user = 1;
var ov milestone =
  for (var post <−− posts)
    where (post.userid == current_user)
      for (var comment <−− comments)
        where (comment.postid == post.id)
          [(user=comment.userid, text=comment.text)];
\end{verbatim}
\end{codelisting}

This code would generate one SQL query to fetch all posts by a user and then as many queries to fetch comments as there are posts. This can be best described as lazy-loading\citep{active-record}, however in this case it's high suboptimal. Performance hit, assuming there are only so many blog posts would not be noticeable, but if a user has 1000 blog posts then fetching comments would require 1001 queries, which is a substantial amount of number, resulting in bad performance of an application. It's surprising that Links doesn't analyse code further than one \textit{for} block, because by looking ahead and realizing that other \textit{for} generators are also reading from database tables', it would be able to create one SQL query by utilizing JOINs. 

Developer is left with no other choice than to either change database structure by avoiding multiple-level generators, which is achieved by de-normalizing\citep{normalization} the data. However, this would return in higher database size and also would require more preparation upfront. Furthermore, if in a long run a performance problem surfaces, it's currently impossible to override a generated SQL query, thus Links is not suitable for applications with big amount of data or complicated data structures as those will make the application too slow to be production-ready. For example Ruby applications allow joins as easily as:

\begin{codelisting}
\begin{verbatim}
Post.join(:comments).each do |p|
   # handle each p (post)
end
\end{verbatim}
\end{codelisting}

\section{NoSQL databases}

Definition of NoSQL states that:

\begin{quote}
NoSQL is a broad class of database management systems that differ from the classic model of the relational database management system (RDBMS) in some significant ways, most important being they do not use SQL as their primary query language
\end{quote}

In practice these databases are usually key-value or document stores, allowing to store structured or unstructured data in collections (databases in RDBMS sense) and query it using various different ways, MapReduce for example. There is a big shift in web industry to move towards them as they tend to be faster and also scale better, which is usually a result of not having ACID\citep{acid} compliance. Which might sound as a drawback, but developers and architects have realised that some of it can be implemented in application level code, while allowing database to be more easily distributed without having to share locks. 

As described above, Links supports a list of RDBMS servers, however it has no support for NoSQL databases. Both PHP and Ruby, as most other languages, already have extensions for them, because the task for them is easier - they only provide connection help and querying mechanisms, not a language integration. However in a same way as relational databases are being handled in Links, NoSQL ones could be handled as well, as semantically NoSQL still store records in a table-like containers. The only problem is that quite a lot of NoSQL databases allow to store documents in unstructured way, meaning two or more documents in a same collection can be of completely different structure. 

In dynamic languages, this is not a problem, as they can just create dynamic objects or arrays and have then contain whatever structure was fetched from a database, but this is a complication in Links. Support dynamic structures in a strict-typed language is not practical and would lose all the benefits Links provides, as explained before, thus I don't think supporting this would be a good idea. Nonetheless, supporting nested documents can be implemented by extending \textit{table} construct or introducing a new one, like such:

\begin{codelisting}
\begin{verbatim}
var posts = collection "posts" 
			  with (name : String, tags : [String], author: (name : String, surname : String)) from db;
\end{verbatim}
\end{codelisting}

\subsection{MapReduce}

Querying data efficiently is a problem and something developers spend quite a lot of time on, hence the above section about slow SQL generation. With highly distributed systems it's even more complicated, as an index might not exists yet thus requiring an ad-hoc query to be indexed first in each of the servers. MapReduce\citep{map-reduce} is a concept which allows to query data across any number of database server nodes efficiently, by defining a mapping function and a reduce function, which take a stream of data and emit some data based on their internal logic. Usually these functions are written in any language supported by that database system and compiled internally for indexing purposes.

For example CouchDB (http://couchdb.apache.org/), one of the more popular NoSQL databases, supports JavaScript out of the box, so this is an example of a JavaScript map and reduce functions which calculates a number of documents having a certain tag, assuming a document is a structured object having a property called \textit{tags} which is an array of strings:

\begin{codelisting}
\begin{verbatim}
// map function
function (doc) { 
  for(var i=0; i<doc.tags.length; i++)  {
    emit(doc.tags[i], 1); 
  }
}

// reduce function
function (tag, counts) {
  var sum=0; 
  for(var i=0; i<counts.length; i++)  {
     sum+=counts[i];
  }
  return sum; 
}
\end{verbatim}
\end{codelisting}

Underlying workings of MapReduce are not in a scope of this project, however I can see them easily implementable in Links. By defining map and reduce functions in Links language developer gains the advantages of syntax checking and typing, and delegates creation of database specific MapReduce creation to Links. For example, a potential implementation of querying using MapReduce in Links might look like this:

\begin{codelisting}
\begin{verbatim}
# map function
fun map_f(doc) {
  for(tag <- doc.tags)
    (tag, 1)
}

# reduce function
fun reduce_f(tag, counts) {
  var sum = sum(counts)
  sum
}

# query operation
for (d <-- documents)
  map (map_f)
  reduce (reduce_f)
    [x]
\end{verbatim}
\end{codelisting}

As seen, this really closely matches with how CouchDB would accept a MapReduce definition to be defined in JavaScript, thus adding support for this, given that Links already knows how to generate JavaScript code, is a feasible addition. Furthermore, even the current database \textit{query} construct is easily portable to NoSQL databases, as \textit{query} defines a predicate each row should return true with, thus it can also be translated into either a MapReduce job or an ad-hoc query if a certain NoSQL database supports that.

\section{Links improvements}

Links is not practical to use for modern application as it's current stage, as it provides a very limited control of generated SQL. Without that an application cannot be optimized thus it can only handle small amounts of data or low traffic. Even though this might be fine for playground applications, trying to implement something more complicated would result in a massive slow application. Thus additional support for \textit{JOIN} hints could be added and it might look something like this:

\begin{codelisting}
\begin{verbatim}
var current_user = 1;
var ov milestone =
  for (var post <−− posts)
    where (post.userid == current_user)
    join (comments)
      for (var comment <−− comments)
        where (comment.postid == post.id)
          [(user=comment.userid, text=comment.text)];
\end{verbatim}
\end{codelisting}

It's now explicitly defined that a join must used, thus a compiler should be able to figure out the two tables to be joined and conditions they have. Adding support for arbitrary SQL queries in a string form doesn't seem to be possible as it would be complicated to strongly-type their results, either requiring to specific that as a list of types or otherwise Links would not be able to handle it.